{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# For feature importance (model-dependent and permutation-based)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To suppress warnings if desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g., reading from CSV – adapt to your actual file names or data sources\n",
    "df_Kiwi = pd.read_csv(\"flight_data_kiwi_allsnaps_clean.csv\")\n",
    "df_Momondo = pd.read_csv(\"flight_data_momondo_All_combined_Cloud.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 271981 entries, 0 to 271980\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   Snap Date      271981 non-null  object\n",
      " 1   Dep Date       271981 non-null  object\n",
      " 2   Return Date    271981 non-null  object\n",
      " 3   Outbound       271981 non-null  object\n",
      " 4   Dep Time       271981 non-null  object\n",
      " 5   Dep Duration   271981 non-null  object\n",
      " 6   Dep Arrival    271981 non-null  object\n",
      " 7   From           271981 non-null  object\n",
      " 8   Stops          271981 non-null  int64 \n",
      " 9   Landing        271981 non-null  object\n",
      " 10  Inbound        271981 non-null  object\n",
      " 11  Back Time      271981 non-null  object\n",
      " 12  Back Duration  271981 non-null  object\n",
      " 13  Back Arrival   271981 non-null  object\n",
      " 14  From.1         271981 non-null  object\n",
      " 15  Stops.1        271981 non-null  int64 \n",
      " 16  Landing.1      271981 non-null  object\n",
      " 17  Dep Company    271981 non-null  object\n",
      " 18  Back Company   271981 non-null  object\n",
      " 19  Price          271981 non-null  object\n",
      "dtypes: int64(2), object(18)\n",
      "memory usage: 41.5+ MB\n",
      "None\n",
      "               Stops        Stops.1\n",
      "count  271981.000000  271981.000000\n",
      "mean        0.047316       0.052776\n",
      "std         0.213299       0.224538\n",
      "min         0.000000       0.000000\n",
      "25%         0.000000       0.000000\n",
      "50%         0.000000       0.000000\n",
      "75%         0.000000       0.000000\n",
      "max         2.000000       2.000000\n",
      "Snap Date        0\n",
      "Dep Date         0\n",
      "Return Date      0\n",
      "Outbound         0\n",
      "Dep Time         0\n",
      "Dep Duration     0\n",
      "Dep Arrival      0\n",
      "From             0\n",
      "Stops            0\n",
      "Landing          0\n",
      "Inbound          0\n",
      "Back Time        0\n",
      "Back Duration    0\n",
      "Back Arrival     0\n",
      "From.1           0\n",
      "Stops.1          0\n",
      "Landing.1        0\n",
      "Dep Company      0\n",
      "Back Company     0\n",
      "Price            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_Kiwi.info())\n",
    "print(df_Kiwi.describe())\n",
    "print(df_Kiwi.isnull().sum())\n",
    "# ... similarly for df_Momondo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 281927 entries, 0 to 281926\n",
      "Data columns (total 17 columns):\n",
      " #   Column                         Non-Null Count   Dtype \n",
      "---  ------                         --------------   ----- \n",
      " 0   Snap Date                      281927 non-null  object\n",
      " 1   Dep Date                       281927 non-null  object\n",
      " 2   Return Date                    281927 non-null  object\n",
      " 3   Dep time                       281927 non-null  object\n",
      " 4   Dep Company                    281927 non-null  object\n",
      " 5   Stops                          281927 non-null  object\n",
      " 6   Dep Duration                   281927 non-null  object\n",
      " 7   From Airport                   281927 non-null  object\n",
      " 8   Landing Airport                281927 non-null  object\n",
      " 9   Back Time                      281927 non-null  object\n",
      " 10  Back Company                   281927 non-null  object\n",
      " 11  Stops.1                        281927 non-null  object\n",
      " 12  Back Duration                  281927 non-null  object\n",
      " 13  (Back Ticket) From Airport     281927 non-null  object\n",
      " 14  (Back Ticket) Landing Airport  281927 non-null  object\n",
      " 15  Price                          281927 non-null  object\n",
      " 16  Class                          281927 non-null  object\n",
      "dtypes: object(17)\n",
      "memory usage: 36.6+ MB\n",
      "None\n",
      "       Snap Date   Dep Date Return Date           Dep time Dep Company  \\\n",
      "count     281927     281927      281927             281927      281927   \n",
      "unique         3         34          38               1206          23   \n",
      "top     3/2/2025  3/16/2025   3/17/2025  2:10 pm – 2:25 pm     easyJet   \n",
      "freq       95750       9829        9750               4117       96434   \n",
      "\n",
      "         Stops Dep Duration From Airport Landing Airport          Back Time  \\\n",
      "count   281927       281927       281927          281927             281927   \n",
      "unique       6          165           88              13               1338   \n",
      "top          0       1h 20m          FCO             FCO  6:45 am – 8:35 am   \n",
      "freq    148617        34157        79286           79219               3608   \n",
      "\n",
      "       Back Company Stops.1 Back Duration (Back Ticket) From Airport  \\\n",
      "count        281927  281927        281927                     281927   \n",
      "unique          171      18           178                        117   \n",
      "top         easyJet       0        1h 20m                        FCO   \n",
      "freq          96611  192975         33187                      78308   \n",
      "\n",
      "       (Back Ticket) Landing Airport   Price    Class  \n",
      "count                         281927  281927   281927  \n",
      "unique                            36     713       10  \n",
      "top                              FCO   $166   Economy  \n",
      "freq                           79028    1828   281728  \n",
      "Snap Date                        0\n",
      "Dep Date                         0\n",
      "Return Date                      0\n",
      "Dep time                         0\n",
      "Dep Company                      0\n",
      "Stops                            0\n",
      "Dep Duration                     0\n",
      "From Airport                     0\n",
      "Landing Airport                  0\n",
      "Back Time                        0\n",
      "Back Company                     0\n",
      "Stops.1                          0\n",
      "Back Duration                    0\n",
      "(Back Ticket) From Airport       0\n",
      "(Back Ticket) Landing Airport    0\n",
      "Price                            0\n",
      "Class                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_Momondo.info())\n",
    "print(df_Momondo.describe())\n",
    "print(df_Momondo.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_additional_features(df):\n",
    "    # Assuming you have columns: 'Dep Date', 'Return Date', 'Snap Date'\n",
    "    # Convert them to datetime if not already\n",
    "    df['Dep Date'] = pd.to_datetime(df['Dep Date'])\n",
    "    df['Return Date'] = pd.to_datetime(df['Return Date'])\n",
    "    df['Snap Date'] = pd.to_datetime(df['Snap Date'])\n",
    "\n",
    "    # Day of week (Monday=0, Sunday=6)\n",
    "    df['outbound_dayofweek'] = df['Dep Date'].dt.dayofweek\n",
    "    df['return_dayofweek'] = df['Return Date'].dt.dayofweek\n",
    "\n",
    "    # Proximity to month-end\n",
    "    df['outbound_day'] = df['Dep Date'].dt.day\n",
    "    df['return_day'] = df['Return Date'].dt.day\n",
    "\n",
    "    # TTT (Time to Travel): difference in days between Snap Date and Dep Date\n",
    "    df['TTT'] = (df['Dep Date'] - df['Snap Date']).dt.days\n",
    "\n",
    "    # LOS (Length of Stay): difference in nights between Dep Date and Return Date\n",
    "    df['LOS'] = (df['Return Date'] - df['Dep Date']).dt.days\n",
    "\n",
    "    # Example: Interaction feature\n",
    "    df['TTT_LOS_interaction'] = df['TTT'] * df['LOS']\n",
    "\n",
    "    # Drop any rows with negative or invalid TTT/LOS if they appear\n",
    "    df = df[df['TTT'] >= 0]\n",
    "    df = df[df['LOS'] > 0]\n",
    "\n",
    "    # Return the updated dataframe\n",
    "    return df\n",
    "\n",
    "df_Kiwi = create_additional_features(df_Kiwi)\n",
    "df_Momondo = create_additional_features(df_Momondo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test Split (Step 3.2)\n",
    "features = [\n",
    "    'TTT', 'LOS', 'outbound_dayofweek', 'return_dayofweek',\n",
    "    'outbound_day', 'return_day', 'TTT_LOS_interaction'\n",
    "    # ... plus any others you decide\n",
    "]\n",
    "target = 'Price'  # adapt if your Price column is named differently\n",
    "\n",
    "X_site1 = df_Kiwi[features].copy()\n",
    "y_site1 = df_Kiwi[target].copy()\n",
    "\n",
    "X_train_s1, X_test_s1, y_train_s1, y_test_s1 = train_test_split(\n",
    "    X_site1, y_site1, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment with Different Normalization Methods (Step 3.8)\n",
    "scalers = {\n",
    "    'none': None,\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler()\n",
    "}\n",
    "def scale_data(X_train, X_test, scaler):\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    else:\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Function & Residual Plot (Steps 3.5 & 3.6)\n",
    "def evaluate_model(model_name, model, y_train, y_pred_train, y_test, y_pred_test, scaler_name=None):\n",
    "    # Calculate metrics\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    # Residual plot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(y_pred_train, y_train - y_pred_train, alpha=0.5, label='Train')\n",
    "    plt.scatter(y_pred_test, y_test - y_pred_test, alpha=0.5, label='Test')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Residual Plot - {model_name} [Scaler={scaler_name}]')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Scaler': scaler_name,\n",
    "        'MSE_Train': mse_train,\n",
    "        'MSE_Test': mse_test,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Train': mae_train,\n",
    "        'MAE_Test': mae_test,\n",
    "        'R2_Train': r2_train,\n",
    "        'R2_Test': r2_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Algorithms (Step 3.1, 3.3)\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, scaler_name=None):\n",
    "    results = []\n",
    "\n",
    "    # 1) LinearRegression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_train = lr.predict(X_train)\n",
    "    y_pred_test = lr.predict(X_test)\n",
    "\n",
    "    results.append(\n",
    "        evaluate_model(\"LinearRegression\", lr, y_train, y_pred_train, y_test, y_pred_test, scaler_name)\n",
    "    )\n",
    "\n",
    "    # 2) DecisionTreeRegressor\n",
    "    # Example of a small param grid; you can expand\n",
    "    for depth in [3, 5, 7]:\n",
    "        dt = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred_train = dt.predict(X_train)\n",
    "        y_pred_test = dt.predict(X_test)\n",
    "\n",
    "        results.append(\n",
    "            evaluate_model(f\"DecisionTree(d={depth})\", dt, y_train, y_pred_train, y_test, y_pred_test, scaler_name)\n",
    "        )\n",
    "\n",
    "    # 3) GaussianProcessRegressor\n",
    "    # Example: try different kernels\n",
    "    kernels = [\n",
    "        DotProduct() + WhiteKernel(),\n",
    "        RBF(length_scale=1.0) + WhiteKernel()\n",
    "    ]\n",
    "    for kernel in kernels:\n",
    "        gpr = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "        gpr.fit(X_train, y_train)\n",
    "        y_pred_train = gpr.predict(X_train)\n",
    "        y_pred_test = gpr.predict(X_test)\n",
    "\n",
    "        results.append(\n",
    "            evaluate_model(f\"GPR({kernel})\", gpr, y_train, y_pred_train, y_test, y_pred_test, scaler_name)\n",
    "        )\n",
    "\n",
    "    # 4) RandomForestRegressor\n",
    "    # Just an example; you could do a loop or GridSearch\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    results.append(\n",
    "        evaluate_model(\"RandomForest\", rf, y_train, rf.predict(X_train), y_test, rf.predict(X_test), scaler_name)\n",
    "    )\n",
    "\n",
    "    # 5) KNeighborsRegressor\n",
    "    for k in [3, 5, 7]:\n",
    "        knn = KNeighborsRegressor(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        results.append(\n",
    "            evaluate_model(f\"KNN(k={k})\", knn, y_train, knn.predict(X_train), y_test, knn.predict(X_test), scaler_name)\n",
    "        )\n",
    "\n",
    "    # 6) GradientBoostingRegressor\n",
    "    gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    results.append(\n",
    "        evaluate_model(\"GBR\", gbr, y_train, gbr.predict(X_train), y_test, gbr.predict(X_test), scaler_name)\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'property' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(RandomForestRegressor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      5\u001b[0m     importances \u001b[38;5;241m=\u001b[39m RandomForestRegressor\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, imp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportances\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimp\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Linear Regression\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'property' object is not iterable"
     ]
    }
   ],
   "source": [
    "#Feature Importance (Step 3.5)\n",
    "# Model-Based\n",
    "# Decision Tree / Random Forest / Gradient Boosting\n",
    "if hasattr(RandomForestRegressor, 'feature_importances_'):\n",
    "    importances = RandomForestRegressor.feature_importances_\n",
    "    for f, imp in zip(features, importances):\n",
    "        print(f\"{f}: {imp:.4f}\")\n",
    "\n",
    "# Linear Regression\n",
    "if isinstance(RandomForestRegressor, LinearRegression):\n",
    "    coeffs = RandomForestRegressor.coef_\n",
    "    for f, c in zip(features, coeffs):\n",
    "        print(f\"{f}: {c:.4f}\")\n",
    "\n",
    "#Permutation Importance (algorithm-agnostic or black-box approach):\n",
    "perm_importance = permutation_importance(RandomForestRegressor, X_test, y_test, n_repeats=10, random_state=42)\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(np.array(features)[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.title(\"Feature Importance (Permutation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '₪ 752'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler_name, scaler_obj \u001b[38;5;129;01min\u001b[39;00m scalers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      5\u001b[0m     X_train_scaled, X_test_scaled \u001b[38;5;241m=\u001b[39m scale_data(X_train_s1, X_test_s1, scaler_obj)\n\u001b[1;32m----> 6\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_s1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler_name\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     all_results_site1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([all_results_site1, results_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Sort or group by best R2_Test or lowest RMSE_Test, etc.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36mtrain_and_evaluate_models\u001b[1;34m(X_train, X_test, y_train, y_test, scaler_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1) LinearRegression\u001b[39;00m\n\u001b[0;32m      6\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m      9\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\liort\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    646\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 648\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\liort\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\liort\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[1;32m-> 1122\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\liort\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     _ensure_no_complex_data(y)\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1147\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '₪ 752'"
     ]
    }
   ],
   "source": [
    "#Running Everything for Site 1 and Site 2 (Step 3.9)\n",
    "all_results_site1 = pd.DataFrame()\n",
    "\n",
    "for scaler_name, scaler_obj in scalers.items():\n",
    "    X_train_scaled, X_test_scaled = scale_data(X_train_s1, X_test_s1, scaler_obj)\n",
    "    results_df = train_and_evaluate_models(\n",
    "        X_train_scaled, X_test_scaled,\n",
    "        y_train_s1, y_test_s1,\n",
    "        scaler_name=scaler_name\n",
    "    )\n",
    "    all_results_site1 = pd.concat([all_results_site1, results_df], ignore_index=True)\n",
    "\n",
    "# Sort or group by best R2_Test or lowest RMSE_Test, etc.\n",
    "all_results_site1.sort_values(by='R2_Test', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing the Distribution of R² Errors (Step 3.10)\n",
    "# Suppose your dataset has columns: 'flight_id' or something that identifies unique flights\n",
    "best_model = ...  # your chosen best model\n",
    "X_test = ...\n",
    "y_test = ...\n",
    "\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Calculate R2 per flight_id\n",
    "df_test_results = pd.DataFrame({\n",
    "    'flight_id': df_Kiwi.loc[X_test.index, 'flight_id'],\n",
    "    'actual': y_test,\n",
    "    'pred': y_pred_test\n",
    "})\n",
    "r2_by_flight = df_test_results.groupby('flight_id').apply(\n",
    "    lambda g: r2_score(g['actual'], g['pred'])\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(r2_by_flight, kde=True)\n",
    "plt.title(\"Distribution of R2 across different flights\")\n",
    "plt.xlabel(\"R2\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Future-Oriented Train/Test Split (Step 3.11)\n",
    "df_train_future = df_Kiwi[df_Kiwi['TTT'] <= 25]\n",
    "df_test_future  = df_Kiwi[df_Kiwi['TTT'] > 25]\n",
    "\n",
    "X_train_future = df_train_future[features]\n",
    "y_train_future = df_train_future[target]\n",
    "\n",
    "X_test_future = df_test_future[features]\n",
    "y_test_future = df_test_future[target]\n",
    "\n",
    "# Possibly scale again\n",
    "X_train_future_scaled, X_test_future_scaled = scale_data(X_train_future, X_test_future, StandardScaler())\n",
    "\n",
    "# Fit best model from earlier or retrain a new one:\n",
    "best_model_future = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "best_model_future.fit(X_train_future_scaled, y_train_future)\n",
    "\n",
    "y_pred_future = best_model_future.predict(X_test_future_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "mse_future = mean_squared_error(y_test_future, y_pred_future)\n",
    "rmse_future = np.sqrt(mse_future)\n",
    "r2_future = r2_score(y_test_future, y_pred_future)\n",
    "print(\"Future Split Results:\")\n",
    "print(f\"MSE: {mse_future}, RMSE: {rmse_future}, R2: {r2_future}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
